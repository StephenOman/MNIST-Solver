{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Mnist Basic - No Bias Signal</h1>\n",
    "\n",
    "A basic classifer to recognize numbers in the MNIST dataset. This is based on the TensorFlow tutorial - but without using TensorFlow or Scikit-Learn.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)\n",
    "```\n",
    "\n",
    "Source: https://www.tensorflow.org/tutorials\n",
    "\n",
    "<h2>No Bias Signal</h2>\n",
    "\n",
    "This particular solver uses the above network configuration (without the Adam optimizer) and also does not use a bias input to each node in the network.\n",
    "\n",
    "<i>Written by Stephen Oman (c) 2021. If you use this code, please include attribution to this GitHub repo. Code is presented as is, for illustration purposes only. Hope you learn something from it!</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Libraries</h2>\n",
    "\n",
    "Load in the libraries needed for this classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import struct\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MNIST Data</h2>\n",
    "\n",
    "MNIST is a commonly used dataset for machine learning. It contains images of hand-written digits. Each image has one digit in the range 0 to 9.\n",
    "\n",
    "There are four files in the MNIST data set:\n",
    "1. train-images-idx3-ubyte - Collection of 60,000 images, each 28x28 pixels. Each pixel has a value representing a greyscale value from 0 (white) to 255 (black).\n",
    "2. train-labels-idx1-ubyte - The associated label for each of the images\n",
    "3. t10k-labels-idx1-ubyte - A set of 10,000 images that should be used only for testing a model\n",
    "4. t10k-images-idx3-ubyte - The corresponding labels for each of the images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the path to the training labels file as necessary\n",
    "train_labels_file = open('mnist_data/train-labels-idx1-ubyte', 'rb')\n",
    "\n",
    "# read in the header from the file\n",
    "label_magic, label_images = struct.unpack('>LL', train_labels_file.read(8))\n",
    "\n",
    "# create a numpy array of the labels\n",
    "train_labels = np.fromfile(train_labels_file, dtype=np.uint8)\n",
    "\n",
    "train_labels_file.close()\n",
    "\n",
    "# now let's import the image data\n",
    "train_data_file = open('mnist_data/train-images-idx3-ubyte', 'rb')\n",
    "\n",
    "# read in the header from the file\n",
    "data_magic, data_images, rows, columns = struct.unpack('>LLLL', train_data_file.read(16))\n",
    "\n",
    "# create a numpy array of the training data\n",
    "train_data = np.fromfile(train_data_file, dtype=np.uint8)\n",
    "train_data = train_data / 255.0\n",
    "train_data_file.close()\n",
    "\n",
    "# now we need to read in the test data\n",
    "test_labels_file = open('mnist_data/t10k-labels-idx1-ubyte', 'rb')\n",
    "\n",
    "# read in the header from the file\n",
    "label_magic, label_images = struct.unpack('>LL', test_labels_file.read(8))\n",
    "\n",
    "# create a numpy array of the labels\n",
    "test_labels = np.fromfile(test_labels_file, dtype=np.uint8)\n",
    "\n",
    "test_labels_file.close()\n",
    "\n",
    "# now let's import the image data\n",
    "test_data_file = open('mnist_data/t10k-images-idx3-ubyte', 'rb')\n",
    "\n",
    "# read in the header from the file\n",
    "data_magic, data_images, rows, columns = struct.unpack('>LLLL', test_data_file.read(16))\n",
    "\n",
    "# create a numpy array of the test data\n",
    "test_data = np.fromfile(test_data_file, dtype=np.uint8)\n",
    "test_data = test_data / 255.0\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Flatten the images</h2>\n",
    "\n",
    "After reading in the data from the file, it is a one-dimensional array of data. We need to split this out into a 2D array, where each row is a separate image and the columns are the 784 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange the data - this has the same effect as the flatten layer in the TensorFlow example\n",
    "# we end up with a vector for each image\n",
    "train_imgs = train_data.reshape(60000, 784)\n",
    "test_imgs = test_data.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Layer 0 - ReLU Function</h2>\n",
    "\n",
    "The first layer of the network uses the Rectified Linear Unit (ReLU) activation. In this implementation, we'll use the leaky version of the ReLU function that outputs a small negative value in the case that the input is less than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU Activation\n",
    "def relu(vector):\n",
    "    epsilon = 0.1\n",
    "    vector[vector<0] *= epsilon\n",
    "    return(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Softmax Function</h3>\n",
    "\n",
    "see https://en.wikipedia.org/wiki/Softmax_function for details\n",
    "\n",
    "$\\sigma(\\textbf{z})_{i}=\\frac{e^{z_{i}}} {\\sum_{j=1}^K e^{z_{j}}}$\n",
    "\n",
    "In this function, we are assuming that <b>z</b> is a 2D matrix, each row is an instance in the mini batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Activation\n",
    "def softmax(z):\n",
    "    ez = np.exp(z)\n",
    "    sum_ez = np.sum(ez, axis = 0)\n",
    "    sm = np.divide(ez, sum_ez)\n",
    "    return (sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>One-Hot Representation of the labels</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                        #\n",
    "# The training labels use integers so we have to convert #\n",
    "# them to one-hot vectors.                               #\n",
    "#                                                        #\n",
    "\n",
    "def one_hot(indexes):\n",
    "    y = np.zeros((indexes.shape[0],10))\n",
    "    for i in range(y.shape[0]):\n",
    "        y[i][indexes[i]] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Categorical Cross Entropy</h3>\n",
    "\n",
    "Categorical Cross Entropy loss function is used when we have a lot of classes that the output could be in, only one of which is correct (i.e. an instance can only be in one class).\n",
    "\n",
    "$-\\sum_{c=0}^{M-1} y_{o,c}\\verb!log!(y_{o,c})$\n",
    "\n",
    "In this case, there are M classes and $y$ is a vector of $y_{o,c}$ probabilities. <b>$y$</b> would be calculated using a softmax function.\n",
    "\n",
    "The Sparse version is used when the classes are numerous and the probability vector <b>p</b> would be mostly full of zeros. In this case, we can pull the correct class using it's label <b>y</b> and just calculate the cost directly from that. Note that we are using the log of the probability, so the higher the probability, the lower the cost.\n",
    "\n",
    "$y$ is a 1D matrix, representing the collection of outputs from softmax and <b>$y$</b> is a vector of the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_entropy(y, y_hat):\n",
    "    return(-1 * y * np.log(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feedforward Network Calculations </h2>\n",
    "\n",
    "This function will calculate the values in the network, feeding the input batch, the weights, and the labels, and then returning those values in a dictionary.\n",
    "\n",
    "The network is straightforward enough:\n",
    "\n",
    "* Layer 0 essentially stacks the bias signals (all ones) onto the images in the input batch (X), then multiplies it by the weights (Z0) and applies the ReLU activation function (A0).\n",
    "* Layer 1 is similar, multiplying the input to the layer by the weights (Z1) and then applying the Softmax activation function (A1).\n",
    "* L is the value from the loss function for each of the images.\n",
    "* J is the average loss for this batch, which is the cost of the model on this set of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs is a tensor representing a batch of images\n",
    "def feedforward(inputs, weights, labels):\n",
    "    \n",
    "    # Layer 0\n",
    "    \n",
    "    X = np.transpose(inputs)\n",
    "    \n",
    "    Z0 = np.dot(np.transpose(weights[\"W0\"]), X)\n",
    "   \n",
    "    A0 = relu(Z0)\n",
    "    \n",
    "    # Layer 1\n",
    "    \n",
    "    Z1 = np.dot(np.transpose(weights[\"W1\"]), A0)\n",
    "    \n",
    "    A1 = softmax(Z1)\n",
    "    \n",
    "    L = x_entropy(np.transpose(labels), A1) # loss function\n",
    "    \n",
    "    J = np.average(L) # cost of this batch\n",
    "    \n",
    "    network = {\n",
    "        \"X\" : X,\n",
    "        \"Z0\" : Z0,\n",
    "        \"A0\" : A0,\n",
    "        \"Z1\" : Z1,\n",
    "        \"A1\" : A1,\n",
    "        \"L\" : L,\n",
    "        \"J\" : J\n",
    "    }\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Backpropogation - Long Form </h2>\n",
    "\n",
    "This is the long form of the backprop algorithm. Essentially, for every function we applied in the feedforward part, we need to work out it's derivative. These derivatives are combined using the Chain Rule.\n",
    "\n",
    "* L_prime is the derivative of the loss function L with respect to it's input A1.\n",
    "* A1_prime is the derivative of the activation function A1 (i.e. the softmax function), with respect to it's input Z1.\n",
    "* Z_prime is the derivative of the summation function with respect to the weights. Since both Z0 and Z1 are the same function in the feedforward network, we can use the same function for the derivative.\n",
    "* ZA_prime is an alternative derivative of the summation function, this time with respect to the input from the previous layer. This is done in order to back-propogate the error into the previous layer. Note that when we call ZA_prime during backprop, we do not include the bias signals, since any error there is not propogated backwards into the previous layer.\n",
    "* A0_prime is the derivative of the activation function in layer 0, i.e. the ReLU function, with respect to summation function Z0.\n",
    "\n",
    "These derivatives are pretty big matricies, since they are calculated by using partial derivatives. They are included here for illustration purposes only. For any decent batch size, they are going to crash the Python kernel as they are too big and take too long to process.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "#                                                                  #\n",
    "# Long form version to see all the derivative chain rule in action #\n",
    "#                                                                  #\n",
    "####################################################################\n",
    "#                                                                  #\n",
    "# Don't use this, as some of the matrices are quite large and will #\n",
    "# crash the kernel, apart from taking forever to churn through it. #\n",
    "#                                                                  #\n",
    "####################################################################\n",
    "\n",
    "# Backprop support functions\n",
    "\n",
    "def L_prime(Y, A1):\n",
    "    print(\"starting L_prime\")\n",
    "    dir_matrix = np.zeros((Y.shape[1],A1.shape[0] * A1.shape[1]))\n",
    "    for h in range(0, Y.shape[1]):\n",
    "        for i in range(0, A1.shape[0]):\n",
    "            if(Y[i][h]==1):\n",
    "                dir_matrix[h][i + (h * A1.shape[0])] = -1.0 / A1[i][h]\n",
    "    print(\"finished L_prime\")\n",
    "    return dir_matrix\n",
    "\n",
    "def A1_prime(A1, Z1):\n",
    "    print(\"starting A1_prime\")\n",
    "    dir_matrix = np.zeros((A1.shape[0] * A1.shape[1], Z1.shape[0] * Z1.shape[1]))\n",
    "    \n",
    "    for h in range(0, A1.shape[1]):\n",
    "        for i in range(0, A1.shape[0]):\n",
    "            for j in range(0, Z1.shape[0]):\n",
    "                if(i == j):\n",
    "                    dir_matrix[i + (h * A1.shape[0])][j + (h * A1.shape[0])] = A1[i][h] * (1 - A1[i][h])\n",
    "                else:\n",
    "                    dir_matrix[i + (h * A1.shape[0])][j + (h * A1.shape[0])] = -1.0 * A1[i][h] * A1[j][h]\n",
    "    print(\"finishing A1_prime\")\n",
    "    return dir_matrix\n",
    "\n",
    "def Z_prime(Z, W, A):\n",
    "    print(\"starting Z_prime\")\n",
    "    dir_matrix = np.zeros((Z.shape[0] * Z.shape[1], W.shape[0]*W.shape[1]))\n",
    "    for h in range(Z.shape[1]):\n",
    "        for i in range(Z.shape[0]):\n",
    "            for j in range(W.shape[1]):\n",
    "                for k in range(W.shape[0]):\n",
    "                    if(i == j):\n",
    "                        dir_matrix[i + (h * Z.shape[0])][(j*W.shape[0]) + k] = A[k][h]\n",
    "    print(\"finishing Z_prime\")\n",
    "    return dir_matrix\n",
    "\n",
    "def ZA_prime(Z, W, A):\n",
    "    print(\"starting ZA_prime\")\n",
    "    dir_matrix = np.zeros((Z.shape[0] * Z.shape[1], A.shape[0] * A.shape[1]))\n",
    "    \n",
    "    for h in range(Z.shape[1]):\n",
    "        for i in range(A.shape[1]):\n",
    "            for j in range(Z.shape[0]):\n",
    "                for k in range(A.shape[0]):\n",
    "                    if(h == i):\n",
    "                        dir_matrix[(h * Z.shape[0]) + j][(i * A.shape[0]) + k] = W[k+1][j]\n",
    "    print(\"finishing ZA_prime\")\n",
    "    return dir_matrix\n",
    "\n",
    "def A0_prime(A,Z):\n",
    "    print(\"starting A0_prime\")\n",
    "    epsilon = 0.1\n",
    "    dir_matrix = np.zeros((A.shape[0] * A.shape[1], Z.shape[0] * Z.shape[1]))\n",
    "    for i in range(0, A.shape[1]):\n",
    "        for j in range(0, A.shape[0]):\n",
    "            if(Z[j][i] < 0):\n",
    "                dir_matrix[(i * A.shape[0]) + j][(i * A.shape[0]) + j] = epsilon\n",
    "            else:\n",
    "                dir_matrix[(i * A.shape[0]) + j][(i * A.shape[0]) + j] = 1\n",
    "    print(\"finishing A0_prime\")\n",
    "    return dir_matrix\n",
    "\n",
    "\n",
    "# main back-propogation chain rule\n",
    "\n",
    "def backprop(inputs, weights, network, labels):\n",
    "    dLdA1 = L_prime(np.transpose(labels), network[\"A1\"])\n",
    "    \n",
    "    dA1dZ1 = A1_prime(network[\"A1\"], network[\"Z1\"])\n",
    "    \n",
    "    dZ1dW1 = Z_prime(network[\"Z1\"], weights[\"W1\"], network[\"A0\"])\n",
    "    \n",
    "    dLdZ1 = np.dot(dLdA1, dA1dZ1)\n",
    "    \n",
    "    dLdW1 = np.dot(dLdZ1, dZ1dW1)\n",
    "    \n",
    "    dJdW1 = np.sum(dLdW1, axis = 0) / inputs.shape[0]\n",
    "    \n",
    "    delta_W1 = 0.05 * np.transpose(dJdW1.reshape((weights[\"W1\"].shape[1], weights[\"W1\"].shape[0])))\n",
    "\n",
    "    dZ1dA0 = ZA_prime(network[\"Z1\"], weights[\"W1\"], network[\"A0\"][1:])\n",
    "    \n",
    "    dLdA0 = np.dot(dLdZ1, dZ1dA0)\n",
    "    \n",
    "    dA0dZ0 = A0_prime(network[\"A0\"][1:], network[\"Z0\"])\n",
    "    \n",
    "    dZ0dW0 = Z_prime(network[\"Z0\"], weights[\"W0\"], network[\"X\"])\n",
    "    \n",
    "    dLdZ0 = np.dot(dLdA0, dA0dZ0)\n",
    "    \n",
    "    dLdW0 = np.dot(dLdZ0, dZ0dW0)\n",
    "    \n",
    "    dJdW0 = np.sum(dLdW0, axis = 0) / inputs.shape[0]\n",
    "    \n",
    "    delta_W0 = 0.05 * np.transpose(dJdW0.reshape((weights[\"W0\"].shape[1], weights[\"W0\"].shape[0])))\n",
    "\n",
    "    return(delta_W0, delta_W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Backprop - Short Form </h2>\n",
    "\n",
    "Luckily for us, with a bit of mathematical tinkering with the formulae above, we can shorten the derivative calculations and implement them without Jacobian matrices.\n",
    "\n",
    "* layer1_prime calcualtes the derivative of the loss function L with respect to the weights in layer 1 (W1).\n",
    "* layer0_prime similarly calcualtes the derivative of the loss function L with respect to the weights in layer 0 (W0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                        #\n",
    "# Shortcut backprop functions - use these instead of the #\n",
    "# longer form functions from above. Your life will be so #\n",
    "# much easier.                                           #\n",
    "#                                                        #\n",
    "\n",
    "#                                                        #\n",
    "# Shortcut to the whole Layer 1 derivative without doing #\n",
    "# Jacobian matrix multiplication.                        #\n",
    "#                                                        #\n",
    "\n",
    "def layer1_prime(W, A, Labels, X):\n",
    "    \n",
    "    A_prime = np.copy(A)\n",
    "    for i in range(labels.shape[0]):\n",
    "        A_prime[labels[i]][i] -= 1   \n",
    "    \n",
    "    dir_matrix = np.dot(X, np.transpose(A_prime))\n",
    "\n",
    "    return (dir_matrix / X.shape[1])\n",
    "\n",
    "#                                                        #\n",
    "# Shortcut to the whole Layer 0 derivative without those #\n",
    "# pesky Jacobians.                                       #\n",
    "#                                                        #\n",
    "\n",
    "\n",
    "def layer0_prime(W1, W0, A1, Labels, X, A0):\n",
    "    epsilon = 0.1\n",
    "    \n",
    "    A1_prime = np.copy(A1)\n",
    "    for i in range(labels.shape[0]):\n",
    "        A1_prime[labels[i]][i] -= 1   \n",
    "        \n",
    "    A0_prime = np.dot(W1, A1_prime)\n",
    "    \n",
    "    Z0_prime = np.copy(A0)\n",
    "    Z0_prime[Z0_prime >= 0] = 1\n",
    "    Z0_prime[Z0_prime <0 ] = epsilon\n",
    "    Z0_prime = Z0_prime * A0_prime\n",
    "\n",
    "    dir_matrix = np.dot(X, np.transpose(Z0_prime))\n",
    "                \n",
    "    return (dir_matrix / X.shape[1])\n",
    "\n",
    "#                                                        #\n",
    "# Call this function to get the gradients for weights in #\n",
    "# both layers.                                           #\n",
    "#                                                        #\n",
    "\n",
    "def backprop_short(inputs, weights, network, labels):\n",
    "    delta_W1 = layer1_prime(weights[\"W1\"], network[\"A1\"], labels, network[\"A0\"])\n",
    "    delta_W0 = layer0_prime(weights[\"W1\"], weights[\"W0\"], network[\"A1\"], labels, \n",
    "                            network[\"X\"], network[\"A0\"])\n",
    "    return(delta_W0, delta_W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Main Learning Driver</h2>\n",
    "\n",
    "This is the controller of the feedforward and backprop functions. It starts with initialising the batch size, learning rate and weights for layer 0 and layer 1.\n",
    "\n",
    "It then goes through the test set 5 times (epochs), each time splitting the test set into batches and submitting them to the feedforward function. Then it executes the backprop function to figure out the changes that must be made to the weights.\n",
    "\n",
    "I've left in some print statements so that you can see the loss decreasing as the algorithm learns (these are commented out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time  134.62833236300003\n",
      "End of Learning with loss :  2.7862858719422323e-05\n"
     ]
    }
   ],
   "source": [
    "t_start = time.perf_counter()\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "learn_rate = 0.1\n",
    "\n",
    "weights_l0 = (2 * (np.random.random([train_imgs[0].shape[0], 512]))) - 1\n",
    "weights_l1 = (2 * (np.random.random([512,10]))) - 1\n",
    "\n",
    "weights = {\n",
    "    \"W0\" : weights_l0,\n",
    "    \"W1\" : weights_l1\n",
    "}\n",
    "\n",
    "for epoch in range(5):\n",
    "    batch_start = 0\n",
    "    batch_end = batch_start + batch_size\n",
    "    batch_num = 1\n",
    "    \n",
    "    # In the case where batch size is set to be\n",
    "    # greater than the number of training images\n",
    "    if(batch_end > train_imgs.shape[0]):\n",
    "        batch_end = train_imgs.shape[0]\n",
    "    \n",
    "    while(batch_start < train_imgs.shape[0]):\n",
    "        inputs = train_imgs[batch_start:batch_end]\n",
    "        labels = train_labels[batch_start:batch_end]\n",
    "        labels_1h = one_hot(labels)\n",
    "\n",
    "        network = feedforward(inputs, weights, labels_1h)\n",
    "        \n",
    "#         if(batch_num % 100 == 0):\n",
    "#             print(\"Epoch : \", epoch, \" Batch : \", batch_num, \" Loss : \", network[\"J\"])\n",
    "        \n",
    "        dW0, dW1 = backprop_short(inputs, weights, network, labels)\n",
    "        \n",
    "        weights[\"W0\"] = weights[\"W0\"] - (learn_rate * dW0)\n",
    "        weights[\"W1\"] = weights[\"W1\"] - (learn_rate * dW1)\n",
    "        \n",
    "        batch_start = batch_start + batch_size\n",
    "        batch_end = batch_end + batch_size\n",
    "        \n",
    "        # check for last batch size being greater\n",
    "        # than the number of remaining instances\n",
    "        \n",
    "        if(batch_end > train_imgs.shape[0]):\n",
    "            batch_end = train_imgs.shape[0]\n",
    "        batch_num = batch_num + 1\n",
    "        \n",
    "t_end = time.perf_counter()\n",
    "print('Elapsed time ', t_end - t_start)            \n",
    "print(\"End of Learning with loss : \", network[\"J\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test A Number</h2>\n",
    "\n",
    "Here's a small section that you can use to select an image in the test set and use the algorithm's feedforward function to make an estimate of what the image represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test a particular value from the test set\n",
    "\n",
    "image = 1234    # Change this to any value in the test set\n",
    "\n",
    "test_img = np.array([test_imgs[image]])\n",
    "test_label = np.array([test_labels[image]])\n",
    "test_label_1h = one_hot(test_label)\n",
    "\n",
    "result = feedforward(test_img, weights, test_label_1h)\n",
    "\n",
    "pixels = test_img.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray_r')\n",
    "heading = \"Image label is \" + str(test_label[0]) + \\\n",
    "    \" \\nand\\n the predicted label is \" + str(np.argmax(result[\"A1\"])) + \"\\n\"\n",
    "plt.title(heading)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Metrics</h2>\n",
    "\n",
    "Using the test set, we can calculate a whole range of metrics so see how good this basic model is:\n",
    "\n",
    "* Total accuracy of the whole model\n",
    "* Accuracy per digit\n",
    "* True Positives, False Positives, True Negatives, False Negatives for each digit\n",
    "* Precision, Recall, Specificity and F1 Score\n",
    "\n",
    "Usually, a basic model such as this one will perform reasonably well, with accuracy in excess of 0.93 (meaning it will make the correct prediction in 93 attempts out of 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions  10000\n",
      "Total correct predictions  9495\n",
      "Overall model accuracy 0.9495 \n",
      "\n",
      "Digit Total  Correct  Accuracy     TP     FN     FP     TN  Precision   Recall  Specificity  F1score\n",
      "  0     980      967   0.98673    967     43   8977     13    0.95743  0.98673      0.99523  0.97186\n",
      "  1    1135     1128   0.99383   1128     49   8816      7    0.95837  0.99383      0.99447  0.97578\n",
      "  2    1032      939   0.90988    939     28   8940     93    0.97104  0.90988      0.99688  0.93947\n",
      "  3    1010      945   0.93564    945     69   8921     65    0.93195  0.93564      0.99232  0.93379\n",
      "  4     982      938   0.95519    938     47   8971     44    0.95228  0.95519      0.99479  0.95374\n",
      "  5     892      842   0.94395    842     65   9043     50    0.92834  0.94395      0.99286  0.93608\n",
      "  6     958      912   0.95198    912     34   9008     46    0.96406  0.95198      0.99624  0.95798\n",
      "  7    1028      960   0.93385    960     31   8941     68    0.96872  0.93385      0.99654  0.95097\n",
      "  8     974      907   0.93121    907     65   8961     67    0.93313  0.93121      0.99280  0.93217\n",
      "  9    1009      957   0.94846    957     74   8917     52    0.92823  0.94846      0.99177  0.93824\n"
     ]
    }
   ],
   "source": [
    "# Basic Metrics\n",
    "\n",
    "# Calculate all the outputs\n",
    "\n",
    "all_test_labels_1h = one_hot(test_labels)\n",
    "\n",
    "test_results = feedforward(test_imgs, weights, all_test_labels_1h)\n",
    "\n",
    "# Stats over all predictions and per digit\n",
    "\n",
    "correct_predictions_per_digit = np.zeros((10,2))\n",
    "\n",
    "for prediction in range(test_results[\"A1\"].shape[1]):\n",
    "    correct_predictions_per_digit[test_labels[prediction]][0] += 1\n",
    "    if(np.argmax(np.transpose(test_results[\"A1\"])[prediction]) == test_labels[prediction]):\n",
    "        correct_predictions_per_digit[test_labels[prediction]][1] += 1\n",
    "        \n",
    "total_predictions = prediction + 1\n",
    "total_correct_predictions = int(np.sum(correct_predictions_per_digit, axis = 0)[1])\n",
    "\n",
    "stats = [{\"tp\":0, \"fp\":0, \"tn\":0, \"fn\":0} for i in range(10)]\n",
    "\n",
    "for stat in range(10):\n",
    "    for prediction in range(test_results[\"A1\"].shape[1]):\n",
    "        if(stat == test_labels[prediction]):\n",
    "            # True Positives & False Negative\n",
    "            if(np.argmax(np.transpose(test_results[\"A1\"])[prediction]) == stat):\n",
    "                stats[stat][\"tp\"] += 1\n",
    "            else:\n",
    "                stats[stat][\"fn\"] += 1\n",
    "        else:\n",
    "            # False Positives and True Negatives\n",
    "            if(np.argmax(np.transpose(test_results[\"A1\"])[prediction]) == stat):\n",
    "                stats[stat][\"fp\"] += 1\n",
    "            else:\n",
    "                stats[stat][\"tn\"] += 1\n",
    "\n",
    "print(\"Total predictions \", total_predictions)\n",
    "print(\"Total correct predictions \", total_correct_predictions)\n",
    "print(\"Overall model accuracy\", total_correct_predictions / total_predictions, \"\\n\")\n",
    "\n",
    "print(\"{0:5} {1:6} {2:8} {3:8} {4:>6} {5:>6} {6:>6} {7:>6} {8:>10} {9:>8} {10:>12} {11:>8}\".format(\n",
    "    \"Digit\", \"Total\", \"Correct\", \"Accuracy\", \"TP\", \"FN\", \"FP\", \"TN\", \n",
    "    \"Precision\", \"Recall\", \"Specificity\", \"F1score\"))\n",
    "\n",
    "for digit in range(10):\n",
    "    total = int(correct_predictions_per_digit[digit][0])\n",
    "    correct = int(correct_predictions_per_digit[digit][1])\n",
    "    accuracy = correct_predictions_per_digit[digit][1] / correct_predictions_per_digit[digit][0]\n",
    "    tp = stats[digit][\"tp\"]\n",
    "    fp = stats[digit][\"fp\"]\n",
    "    tn = stats[digit][\"tn\"]\n",
    "    fn = stats[digit][\"fn\"]\n",
    "    precision =  tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1score = (tp * 2) / ((tp * 2) + fp + fn)\n",
    "    \n",
    "    print(\"{0:^5} {1:5} {2:8} {3:9.5f} {4:6} {5:6} {6:6} {7:6} {8:10.5f} {9:8.5f} {10:12.5f} {11:8.5f}\"\n",
    "          .format(digit, total, correct, accuracy, tp, fp, tn, fn, \n",
    "                  precision, recall, specificity, f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
